project: thrifty-c
method: bayes
metric:
  name: reward
  goal: maximize
parameters:
#  rollout_round_min_episodes:
#    values: [1, 3, 5, 7, 10, 12, 15]
#  rollout_round_min_timesteps:
#    values: [600]
  iters:
    values: [7, 8, 9, 10, 15, 20, 25, 30, 35, 40]
  layer_size:
    values: [32, 64, 128, 256]
  num_layers:
    values: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]
  bc_episodes:
    values: [5, 10, 15, 20, 25, 35, 45, 50, 60]
  num_nets:
    values: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
  grad_steps:
    values: [100, 200, 300, 400, 500, 600, 700, 800, 900]
  pi_lr:
    values: [ 1e-4, 5e-4, 1e-3, 5e-3, 1e-2, 4, 5, 6, 7, 8, 9, 10 ]
  bc_epochs:
    values: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
  batch_size:
    values: [32, 64, 100, 128, 256]
  obs_per_iter:
    values: [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400]
  target_rate:
    values: [0.01, 0.02, 0.03, 0.05, 0.08, 0.1, 0.12, 0.15, 0.18, 0.2]
  num_test_episodes:
    values: [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 17, 20]
  gamma:
    values: [0.99, 0.999, 0.9999, 0.99999, 0.999999]
  activation_fn:
    values: [ "Tanh", "ReLU" ]


program: train-dagger
command:
  - ${program}
  - "--daggerAlgorithm"
  - "thrifty"
  - "--inp_dir"
  - "training_data"
  - "--env"
  - "training_data/env_window_2robots.yaml"
  - "--model_path"
  - "deps/dynobench/models/point_2.yaml"
  - "--validate"
